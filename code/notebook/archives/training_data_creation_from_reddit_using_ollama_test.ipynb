{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.reddit_search.tool import RedditSearchRun\n",
    "from langchain_community.utilities.reddit_search import RedditSearchAPIWrapper\n",
    "from langchain_community.tools.reddit_search.tool import RedditSearchSchema\n",
    "from langchain_community.llms import Ollama\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from utility import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(file_path):\n",
    "    \"\"\"\n",
    "    Load questions from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing questions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of questions.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            questions.append(row['question'])\n",
    "    return questions\n",
    "\n",
    "\n",
    "def search_reddit(query, sort_by=\"relevance\", time_filter=\"all\", subreddit=\"EhlersDanlos\", limit=3):\n",
    "    \"\"\"\n",
    "    Search Reddit for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to use.\n",
    "        sort_by (str, optional): The sorting method for the search results.\n",
    "            Accepted values are \"relevance\", \"hot\", \"top\", \"new\", and \"comments\".\n",
    "        time_filter (str, optional): The time filter for the search results.\n",
    "            Accepted values are \"all\", \"year\", \"month\", \"week\", \"day\", and \"hour\".\n",
    "        subreddit (str, optional): The subreddit to search in.\n",
    "        limit (int, optional): The maximum number of search results to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the search results from Reddit.\n",
    "    \"\"\"\n",
    "    search_params = RedditSearchSchema(\n",
    "        query=query,\n",
    "        sort=sort_by,\n",
    "        time_filter=time_filter,\n",
    "        subreddit=subreddit,\n",
    "        limit=limit\n",
    "    )\n",
    "    result = search.run(tool_input=search_params.dict())\n",
    "\n",
    "    # Convert the result string into a list of dictionaries\n",
    "    posts = []\n",
    "    if isinstance(result, str):\n",
    "        result_lines = result.split(\"\\n\\nPost Title: '\")\n",
    "\n",
    "        for i, post_str in enumerate(result_lines):\n",
    "            if i == 0:\n",
    "                # This is the first chunk, handle separately to remove leading text\n",
    "                post_str = post_str.split(\"Post Title: '\", 1)[-1]\n",
    "            \n",
    "            lines = post_str.strip().split(\"\\n\")\n",
    "            post = {}\n",
    "\n",
    "            # Reattach 'Post Title:' for the first line\n",
    "            if lines:\n",
    "                post[\"Post Title\"] = lines[0].strip(\"'\")\n",
    "\n",
    "            # Combine lines for the text body\n",
    "            text_body = []\n",
    "            for line in lines[1:]:\n",
    "                line = line.strip()\n",
    "                if \": \" in line:\n",
    "                    key, value = line.split(\": \", 1)\n",
    "                    if key.strip() == \"Text body\":\n",
    "                        text_body.append(value.strip())\n",
    "                    else:\n",
    "                        post[key.strip()] = value.strip()\n",
    "                else:\n",
    "                    text_body.append(line.strip())\n",
    "            \n",
    "            # Join the text body\n",
    "            post[\"Text body\"] = \" \".join(text_body).strip()\n",
    "            \n",
    "            posts.append(post)\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def process_post(question, post):\n",
    "    \"\"\"\n",
    "    Confirm relevance of a post and generate structured output using LLM.\n",
    "    Args:\n",
    "        question (str): The question to confirm relevance against.\n",
    "        post (dict): The Reddit post to process.\n",
    "    Returns:\n",
    "        str or None: A summarized response if relevant, otherwise None.\n",
    "    \"\"\"\n",
    "    instruction = f\"Verify if the following post addressed the question: '{question}'? If yes, say 'Yes, it is related to the question' and provide a summarized answer. If not, say 'No, it is not related to the question'.\"\n",
    "    input_text = post.get(\"Text body\", \"\")\n",
    "    prompt = f\"Instruction: {instruction}\\n\\nInput: {input_text}\\n\\nOutput:\"\n",
    "    \n",
    "    relevance_system_prompt = \"You are an excellent question validator. Verify if the post addresses the question asked\"\n",
    "    try:\n",
    "        response = get_GPT_response(prompt, relevance_system_prompt, os.environ.get('MODEL_NAME'), temperature=0.3)\n",
    "    except:\n",
    "        response = 'No, it is not related to the question'\n",
    "    \n",
    "    if \"yes, it is related to the question\" in response.lower():\n",
    "        summarized_response = response.split(\"Summarized answer:\")[-1].strip() if \"Summarized answer:\" in response else response.split(\"Output:\")[-1].strip()\n",
    "        return summarized_response\n",
    "    return None\n",
    "\n",
    "def combine_responses(question, responses):\n",
    "    \"\"\"\n",
    "    Combine multiple responses into a single summarized answer using LLM.\n",
    "    Args:\n",
    "        question (str): The question being answered.\n",
    "        responses (list): A list of individual summarized responses.\n",
    "    Returns:\n",
    "        str: A single summarized answer.\n",
    "    \"\"\"\n",
    "    combined_input = \" \".join(responses)\n",
    "    summary_system_prompt = '''\n",
    "    You are an expert who summarizes the textual passages elegantly. \n",
    "    In your summary, focus on the relevant information and avoid using phrases like 'the author of the post reported'. Instead, provide a summary of the key points.\n",
    "    '''\n",
    "    prompt = f\"Instruction: Based on discussions on online platforms like Reddit, summarize the following responses to the question: '{question}'. \\n\\nInput: {combined_input}\\n\\nOutput:\"\n",
    "    try:\n",
    "        combined_response = get_GPT_response(prompt, summary_system_prompt, os.environ.get('MODEL_NAME'), temperature=0.3)\n",
    "    except:\n",
    "        combined_response = combined_input\n",
    "\n",
    "    return combined_response.strip()\n",
    "\n",
    "def search_and_process_questions(questions, delay=1):\n",
    "    \"\"\"\n",
    "    Search Reddit for each question and process the results.\n",
    "\n",
    "    Args:\n",
    "        questions (list): A list of questions to search and process.\n",
    "        delay (int): Delay in seconds between API requests to avoid rate limiting.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing processed results.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "\n",
    "    for question in tqdm(questions):\n",
    "        reddit_results = search_reddit(question)\n",
    "        relevant_responses = []\n",
    "        for post in reddit_results:\n",
    "            processed_post = process_post(question, post)\n",
    "            if processed_post:\n",
    "                relevant_responses.append(processed_post)\n",
    "        \n",
    "        if relevant_responses:\n",
    "            combined_response = combine_responses(question, relevant_responses)\n",
    "            final_results.append({\n",
    "                \"input\":\"\",\n",
    "                \"instruction\": question,\n",
    "                \"output\": combined_response\n",
    "            })\n",
    "        \n",
    "        # Sleep to avoid hitting the API rate limit\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "\n",
    "def save_results_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the processed results to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to save to JSON.\n",
    "        file_path (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions from the CSV file\n",
    "questions = load_questions('../../eds_reddit/eds_questions_llm_generated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try only 10\n",
    "# questions = questions[:50]\n",
    "# questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load environment variables (save a .eds.env file with your reddit credentials in the repo root folder)\n",
    "dotenv_path = os.path.join(REPO_ROOT_PATH, '.eds.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "client_id = os.environ.get('client_id')\n",
    "client_secret = os.environ.get('client_secret')\n",
    "user_agent = os.environ.get('user_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit Search API wrapper\n",
    "search = RedditSearchRun(\n",
    "    api_wrapper=RedditSearchAPIWrapper(\n",
    "        reddit_client_id=client_id,\n",
    "        reddit_client_secret=client_secret,\n",
    "        reddit_user_agent=user_agent,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 100/100 [05:17<00:00,  3.17s/it]\n"
     ]
    }
   ],
   "source": [
    "final_results = search_and_process_questions(questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final output to JSON file:\n",
    "# save_results_to_json(final_results, '../../eds_reddit/final_output.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': '',\n",
       " 'instruction': 'Can EDS lead to an increased risk of developing anxiety or panic disorders?',\n",
       " 'output': \"The post discusses an individual's anxiety and fear related to their EDS and the potential increased risk of infection or difficulty healing. They also express concerns about using laughing gas due to their dizziness and dysautonomia.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
