{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.tools.reddit_search.tool import RedditSearchRun\n",
    "from langchain_community.utilities.reddit_search import RedditSearchAPIWrapper\n",
    "from langchain_community.tools.reddit_search.tool import RedditSearchSchema\n",
    "from langchain_openai.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_questions(file_path):\n",
    "    \"\"\"\n",
    "    Load questions from a CSV file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file containing questions.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of questions.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    with open(file_path, newline='') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            questions.append(row['question'])\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load questions from the CSV file\n",
    "questions = load_questions('../../eds_data/llm_generated_questions/eds_questions_llm_generated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load environment variables\n",
    "dotenv_path = os.path.join('/Users/mariacatalinavilloutareyes/dev', '.eds.env')\n",
    "load_dotenv(dotenv_path)\n",
    "\n",
    "client_id = os.environ.get('client_id')\n",
    "client_secret = os.environ.get('client_secret')\n",
    "user_agent = os.environ.get('user_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Reddit Search API wrapper\n",
    "search = RedditSearchRun(\n",
    "    api_wrapper=RedditSearchAPIWrapper(\n",
    "        reddit_client_id=client_id,\n",
    "        reddit_client_secret=client_secret,\n",
    "        reddit_user_agent=user_agent,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_reddit(query, sort_by=\"relevance\", time_filter=\"all\", subreddit=\"EhlersDanlos\", limit=3):\n",
    "    \"\"\"\n",
    "    Search Reddit for a given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query to use.\n",
    "        sort_by (str, optional): The sorting method for the search results.\n",
    "            Accepted values are \"relevance\", \"hot\", \"top\", \"new\", and \"comments\".\n",
    "        time_filter (str, optional): The time filter for the search results.\n",
    "            Accepted values are \"all\", \"year\", \"month\", \"week\", \"day\", and \"hour\".\n",
    "        subreddit (str, optional): The subreddit to search in.\n",
    "        limit (int, optional): The maximum number of search results to return.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing the search results from Reddit.\n",
    "    \"\"\"\n",
    "    search_params = RedditSearchSchema(\n",
    "        query=query,\n",
    "        sort=sort_by,\n",
    "        time_filter=time_filter,\n",
    "        subreddit=subreddit,\n",
    "        limit=limit\n",
    "    )\n",
    "    result = search.run(tool_input=search_params.dict())\n",
    "\n",
    "    # Convert the result string into a list of dictionaries\n",
    "    posts = []\n",
    "    if isinstance(result, str):\n",
    "        result_lines = result.split(\"\\n\\nPost Title: '\")\n",
    "\n",
    "        for i, post_str in enumerate(result_lines):\n",
    "            if i == 0:\n",
    "                # This is the first chunk, handle separately to remove leading text\n",
    "                post_str = post_str.split(\"Post Title: '\", 1)[-1]\n",
    "            \n",
    "            lines = post_str.strip().split(\"\\n\")\n",
    "            post = {}\n",
    "\n",
    "            # Reattach 'Post Title:' for the first line\n",
    "            if lines:\n",
    "                post[\"Post Title\"] = lines[0].strip(\"'\")\n",
    "\n",
    "            # Combine lines for the text body\n",
    "            text_body = []\n",
    "            for line in lines[1:]:\n",
    "                line = line.strip()\n",
    "                if \": \" in line:\n",
    "                    key, value = line.split(\": \", 1)\n",
    "                    if key.strip() == \"Text body\":\n",
    "                        text_body.append(value.strip())\n",
    "                    else:\n",
    "                        post[key.strip()] = value.strip()\n",
    "                else:\n",
    "                    text_body.append(line.strip())\n",
    "            \n",
    "            # Join the text body\n",
    "            post[\"Text body\"] = \" \".join(text_body).strip()\n",
    "            \n",
    "            posts.append(post)\n",
    "    \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_post(question, post):\n",
    "    \"\"\"\n",
    "    Confirm relevance of a post and generate structured output using LLM.\n",
    "    Args:\n",
    "        question (str): The question to confirm relevance against.\n",
    "        post (dict): The Reddit post to process.\n",
    "    Returns:\n",
    "        str or None: A summarized response if relevant, otherwise None.\n",
    "    \"\"\"\n",
    "    instruction = f\"Please analyze the following Reddit post and determine if it is related to the question: '{question}'. If it is related, respond with 'Yes, the post is related to the question' and provide a concise summary of the relevant information, focusing on the general experiences or insights shared rather than attributing them to specific individuals. When summarizing, use language that is sensitive and respectful to all people, and avoid making direct references to the post's author or any individuals mentioned. If the post is not related to the question, simply respond with 'No, the post is not related to the question'.\"\n",
    "    input_text = post.get(\"Text body\", \"\")\n",
    "    prompt = f\"Instruction: {instruction}\\n\\nInput: {input_text}\\n\\nOutput:\"\n",
    "    \n",
    "    response = model.invoke(prompt)\n",
    "    response_text = response.content\n",
    "    \n",
    "    if \"yes, the post is related to the question\" in response_text.lower():\n",
    "        summarized_response = response_text.split(\"Summarized answer:\")[-1].strip() if \"Summarized answer:\" in response_text else response_text.split(\"Output:\")[-1].strip()\n",
    "        return summarized_response\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_responses(question, responses):\n",
    "    \"\"\"\n",
    "    Combine multiple responses into a single summarized answer using LLM.\n",
    "    Args:\n",
    "        question (str): The question being answered.\n",
    "        responses (list): A list of individual summarized responses.\n",
    "    Returns:\n",
    "        str: A single summarized answer.\n",
    "    \"\"\"\n",
    "    combined_input = \" \".join(responses)\n",
    "    prompt = f\"Instruction: Based on discussions on online platforms like Reddit, summarize the following responses to the question: '{question}'. In your summary, avoid using phrases like 'the author of the post or individual reported'.\\n\\nInput: {combined_input}\\n\\nOutput:\"\n",
    "\n",
    "    combined_response = model.invoke(prompt)\n",
    "    combined_response_text = combined_response.content\n",
    "\n",
    "    return combined_response_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_output(output):\n",
    "    \"\"\"\n",
    "    Generate a concise summary of the output.\n",
    "\n",
    "    Args:\n",
    "        output (str): Detailed output text to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        str: A short summary of the output.\n",
    "    \"\"\"\n",
    "    prompt = f\"Instruction: Summarize the following text into a concise statement.\\n\\nInput: {output}\\n\\nOutput:\"\n",
    "    summary_response = model.invoke(prompt)\n",
    "    summary_text = summary_response.content\n",
    "\n",
    "    return summary_text.split(\"Output:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_and_process_questions(questions, delay=1):\n",
    "    \"\"\"\n",
    "    Search Reddit for each question and process the results.\n",
    "\n",
    "    Args:\n",
    "        questions (list): A list of questions to search and process.\n",
    "        delay (int): Delay in seconds between API requests to avoid rate limiting.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing processed results.\n",
    "    \"\"\"\n",
    "    final_results = []\n",
    "\n",
    "    for question in questions:\n",
    "        reddit_results = search_reddit(question)\n",
    "        relevant_responses = []\n",
    "        for post in reddit_results:\n",
    "            processed_post = process_post(question, post)\n",
    "            if processed_post:\n",
    "                relevant_responses.append(processed_post)\n",
    "        \n",
    "        if relevant_responses:\n",
    "            combined_response = combine_responses(question, relevant_responses)\n",
    "            input_summary = summarize_output(combined_response)\n",
    "            final_results.append({\n",
    "                \"input\": input_summary,\n",
    "                \"instruction\": question,\n",
    "                \"output\": combined_response\n",
    "            })\n",
    "        \n",
    "        # Sleep to avoid hitting the API rate limit\n",
    "        time.sleep(delay)\n",
    "\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the processed results to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to save to JSON.\n",
    "        file_path (str): Path to the output JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as jsonfile:\n",
    "        json.dump(data, jsonfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = search_and_process_questions(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final output to JSON file:\n",
    "save_results_to_json(final_results, '../../eds_data/reddit_data/reddit_ouput_generation.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 out of 602 questions\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(final_results)} out of 602 questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
