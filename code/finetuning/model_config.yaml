LLM_MODEL_NAME : 'meta-llama/Meta-Llama-3-8B-Instruct'
LLM_MODEL_BRANCH : 'main'
LLM_NAME : 'Meta-Llama-3-8B-Instruct'
LLM_CACHE_DIR : '/home/ubuntu/.hf_cache'
LLM_TEMPERATURE : 0.7
MAX_SEQ_LENGTH : 512

RAG_EMBEDDING_MODEL : 'text-embedding-ada-002'
RAG_PINECONE_INDEX : 'eds'

TRAIN_DATA_PATH : '/home/ubuntu/llm_for_eds_ks_branch/llm_for_eds/eds_data/rare_disease_eds_data.json'
TEST_DATA_RATIO : 0.15
TEST_BATCH_SIZE : 8
HACKATHON_TEST_DATA : '/home/ubuntu/llm_for_eds_ks_branch/llm_for_eds/eds_data/hackathon_test_questions.jsonl'

LORA_r : 16
LORA_alpha_fraction : 1
LORA_dropout : 0.01
LORA_target_modules : ["q_proj", "k_proj", "v_proj"]

MODEL_OUTDIR : '/home/ubuntu/finetuning_outputs'
CHECKPOINT_SAVE_DIR_NAME : 'eds_llama3_checkpoints'
LOGGING_DIR : '/home/ubuntu/finetuning_logs'
TRAIN_EPOCHS : 2
BATCH_SIZE_PER_GPU_FOR_TRAINING : 4
LEARNING_RATE : 1e-4
GRADIENT_ACCUMULATION_STEPS : 1
OPTIMIZER : 'paged_adamw_32bit'
SAVE_STRATEGY : 'steps'
SAVE_STEPS : 250
LOGGING_STEPS : 250
LR_SCHEDULER_TYPE : 'constant'
MAX_STEPS : -1
WARMUP_RATIO : 0
MAX_GRAD_NORM : 1

WANDB_PROJECT_NAME : 'eds_llama_3_finetuning'
WANDDB_RUN_NAME : 'eds_llama_3_finetuning_on_Jun_12_2024_v1'
